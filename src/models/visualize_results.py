"""
Comprehensive Visualization Script for Sleep Health Ensemble Models
Creates charts and graphs to analyze model performance and insights
"""

import pandas as pd
import numpy as np
import matplotlib.pyplot as plt
import seaborn as sns
import joblib
import json
from pathlib import Path
import sys
import os
from sklearn.metrics import confusion_matrix, roc_curve, auc, classification_report
from sklearn.metrics import mean_squared_error, r2_score
from sklearn.model_selection import learning_curve
import warnings
warnings.filterwarnings('ignore')

# Add path for imports
sys.path.append(os.path.join(os.path.dirname(__file__), '../../config'))
try:
    from config import *
except ImportError:
    PROJECT_ROOT = Path(__file__).parent.parent.parent
    PROCESSED_DATA_DIR = PROJECT_ROOT / "data" / "processed"
    MODELS_DIR = PROJECT_ROOT / "src" / "models"

from ensemble_models import SleepHealthEnsemble

# Set up plotting style
plt.style.use('default')
sns.set_palette("husl")
plt.rcParams['figure.figsize'] = [12, 8]
plt.rcParams['font.size'] = 11

def load_data_and_models():
    """Load dataset and trained models"""
    
    # Load data
    sleep_df = pd.read_csv(PROCESSED_DATA_DIR / 'sleep_health_cleaned.csv')
    
    # Load ensemble
    ensemble = SleepHealthEnsemble(random_state=42)
    
    # Load models
    model_files = {
        'xgboost': 'xgboost_model.joblib',
        'random_forest': 'random_forest_model.joblib', 
        'logistic': 'logistic_model.joblib',
        'xgboost_regressor': 'xgboost_regressor_model.joblib',
        'rf_regressor': 'rf_regressor_model.joblib'
    }
    
    for model_name, filename in model_files.items():
        model_path = MODELS_DIR / filename
        if model_path.exists():
            ensemble.models[model_name] = joblib.load(model_path)
    
    # Load preprocessors
    preprocessor_path = MODELS_DIR / "preprocessors.joblib"
    if preprocessor_path.exists():
        ensemble.preprocessors = joblib.load(preprocessor_path)
    
    # Load performance metrics
    metrics_path = MODELS_DIR / "performance_metrics.json"
    metrics = {}
    if metrics_path.exists():
        with open(metrics_path) as f:
            metrics = json.load(f)
    
    # Load feature importance
    importance_path = MODELS_DIR / "feature_importance.json"
    importance = {}
    if importance_path.exists():
        with open(importance_path) as f:
            importance = json.load(f)
    
    return sleep_df, ensemble, metrics, importance

def plot_model_performance_comparison(metrics):
    """Plot comparison of model performances"""
    
    fig, axes = plt.subplots(2, 2, figsize=(16, 12))
    fig.suptitle('Sleep Health Ensemble Model Performance Comparison', fontsize=16, fontweight='bold')
    
    # Classification Accuracy
    if 'classification' in metrics:
        models = list(metrics['classification'].keys())
        accuracies = [metrics['classification'][model]['test_accuracy'] for model in models]
        f1_scores = [metrics['classification'][model]['test_f1'] for model in models]
        auc_scores = [metrics['classification'][model].get('test_auc', 0) for model in models]
        
        model_names = [model.replace('_', ' ').title() for model in models]
        
        # Accuracy comparison
        bars1 = axes[0, 0].bar(model_names, accuracies, color=['#1f77b4', '#ff7f0e', '#2ca02c'], alpha=0.8)
        axes[0, 0].set_title('Classification Accuracy', fontweight='bold')
        axes[0, 0].set_ylabel('Accuracy')
        axes[0, 0].set_ylim(0.85, 1.0)
        
        # Add value labels on bars
        for bar, acc in zip(bars1, accuracies):
            axes[0, 0].text(bar.get_x() + bar.get_width()/2, bar.get_height() + 0.005, 
                           f'{acc:.1%}', ha='center', va='bottom', fontweight='bold')
        
        # F1 Score comparison
        bars2 = axes[0, 1].bar(model_names, f1_scores, color=['#1f77b4', '#ff7f0e', '#2ca02c'], alpha=0.8)
        axes[0, 1].set_title('F1 Score', fontweight='bold')
        axes[0, 1].set_ylabel('F1 Score')
        axes[0, 1].set_ylim(0.85, 1.0)
        
        for bar, f1 in zip(bars2, f1_scores):
            axes[0, 1].text(bar.get_x() + bar.get_width()/2, bar.get_height() + 0.005, 
                           f'{f1:.1%}', ha='center', va='bottom', fontweight='bold')
    
    # Regression Performance
    if 'regression' in metrics:
        reg_models = list(metrics['regression'].keys())
        rmse_scores = [metrics['regression'][model]['test_rmse'] for model in reg_models]
        r2_scores = [metrics['regression'][model]['test_r2'] for model in reg_models]
        
        reg_model_names = [model.replace('_regressor', '').replace('_', ' ').title() for model in reg_models]
        
        # RMSE comparison
        bars3 = axes[1, 0].bar(reg_model_names, rmse_scores, color=['#d62728', '#9467bd'], alpha=0.8)
        axes[1, 0].set_title('Regression RMSE (Lower is Better)', fontweight='bold')
        axes[1, 0].set_ylabel('RMSE')
        
        for bar, rmse in zip(bars3, rmse_scores):
            axes[1, 0].text(bar.get_x() + bar.get_width()/2, bar.get_height() + 0.002, 
                           f'{rmse:.3f}', ha='center', va='bottom', fontweight='bold')
        
        # R¬≤ comparison
        bars4 = axes[1, 1].bar(reg_model_names, r2_scores, color=['#d62728', '#9467bd'], alpha=0.8)
        axes[1, 1].set_title('R¬≤ Score (Higher is Better)', fontweight='bold')
        axes[1, 1].set_ylabel('R¬≤ Score')
        axes[1, 1].set_ylim(0.95, 1.0)
        
        for bar, r2 in zip(bars4, r2_scores):
            axes[1, 1].text(bar.get_x() + bar.get_width()/2, bar.get_height() + 0.001, 
                           f'{r2:.1%}', ha='center', va='bottom', fontweight='bold')
    
    plt.tight_layout()
    return fig

def plot_feature_importance(importance):
    """Plot feature importance from different models"""
    
    if not importance:
        return None
    
    fig, axes = plt.subplots(1, 2, figsize=(18, 8))
    fig.suptitle('Feature Importance Analysis', fontsize=16, fontweight='bold')
    
    # XGBoost importance
    if 'xgboost' in importance:
        xgb_features = list(importance['xgboost'].keys())
        xgb_importances = list(importance['xgboost'].values())
        
        # Sort by importance
        sorted_idx = np.argsort(xgb_importances)[::-1]
        sorted_features = [xgb_features[i] for i in sorted_idx]
        sorted_importances = [xgb_importances[i] for i in sorted_idx]
        
        # Plot top 10 features
        top_n = min(10, len(sorted_features))
        y_pos = np.arange(top_n)
        
        bars1 = axes[0].barh(y_pos, sorted_importances[:top_n], color='skyblue', alpha=0.8)
        axes[0].set_yticks(y_pos)
        axes[0].set_yticklabels(sorted_features[:top_n])
        axes[0].set_xlabel('Importance')
        axes[0].set_title('XGBoost Feature Importance', fontweight='bold')
        axes[0].invert_yaxis()
        
        # Add value labels
        for i, (bar, imp) in enumerate(zip(bars1, sorted_importances[:top_n])):
            axes[0].text(bar.get_width() + 0.01, bar.get_y() + bar.get_height()/2, 
                        f'{imp:.1%}', va='center', fontweight='bold')
    
    # Random Forest importance
    if 'random_forest' in importance:
        rf_features = list(importance['random_forest'].keys())
        rf_importances = list(importance['random_forest'].values())
        
        # Sort by importance
        sorted_idx = np.argsort(rf_importances)[::-1]
        sorted_features = [rf_features[i] for i in sorted_idx]
        sorted_importances = [rf_importances[i] for i in sorted_idx]
        
        # Plot top 10 features
        top_n = min(10, len(sorted_features))
        y_pos = np.arange(top_n)
        
        bars2 = axes[1].barh(y_pos, sorted_importances[:top_n], color='lightcoral', alpha=0.8)
        axes[1].set_yticks(y_pos)
        axes[1].set_yticklabels(sorted_features[:top_n])
        axes[1].set_xlabel('Importance')
        axes[1].set_title('Random Forest Feature Importance', fontweight='bold')
        axes[1].invert_yaxis()
        
        # Add value labels
        for i, (bar, imp) in enumerate(zip(bars2, sorted_importances[:top_n])):
            axes[1].text(bar.get_width() + 0.01, bar.get_y() + bar.get_height()/2, 
                        f'{imp:.1%}', va='center', fontweight='bold')
    
    plt.tight_layout()
    return fig

def plot_confusion_matrices(sleep_df, ensemble, metrics):
    """Plot confusion matrices for classification models"""
    
    # Prepare data
    X, y = ensemble.preprocess_data(sleep_df, target_col='Sleep Disorder', fit_preprocessors=True)
    
    from sklearn.model_selection import train_test_split
    X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42, stratify=y)
    
    # Get class names
    class_names = ensemble.preprocessors['target_encoder'].classes_
    
    fig, axes = plt.subplots(1, 3, figsize=(18, 5))
    fig.suptitle('Confusion Matrices - Sleep Disorder Classification', fontsize=16, fontweight='bold')
    
    model_names = ['xgboost', 'random_forest', 'logistic']
    model_titles = ['XGBoost', 'Random Forest', 'Logistic Regression']
    
    for i, (model_name, title) in enumerate(zip(model_names, model_titles)):
        if model_name in ensemble.models:
            model = ensemble.models[model_name]
            y_pred = model.predict(X_test)
            
            cm = confusion_matrix(y_test, y_pred)
            cm_normalized = cm.astype('float') / cm.sum(axis=1)[:, np.newaxis]
            
            # Plot confusion matrix
            im = axes[i].imshow(cm_normalized, interpolation='nearest', cmap='Blues')
            axes[i].set_title(f'{title}\\nAccuracy: {metrics["classification"][model_name]["test_accuracy"]:.1%}', 
                             fontweight='bold')
            
            # Add colorbar
            cbar = plt.colorbar(im, ax=axes[i], fraction=0.046, pad=0.04)
            cbar.set_label('Normalized Confusion Matrix')
            
            # Add text annotations
            thresh = cm_normalized.max() / 2.
            for j in range(cm.shape[0]):
                for k in range(cm.shape[1]):
                    axes[i].text(k, j, f'{cm[j, k]}\\n({cm_normalized[j, k]:.1%})',
                               ha="center", va="center",
                               color="white" if cm_normalized[j, k] > thresh else "black",
                               fontweight='bold')
            
            axes[i].set_ylabel('True Label')
            axes[i].set_xlabel('Predicted Label')
            axes[i].set_xticks(range(len(class_names)))
            axes[i].set_yticks(range(len(class_names)))
            axes[i].set_xticklabels(class_names, rotation=45)
            axes[i].set_yticklabels(class_names)
    
    plt.tight_layout()
    return fig

def plot_roc_curves(sleep_df, ensemble):
    """Plot ROC curves for classification models"""
    
    # Prepare data
    X, y = ensemble.preprocess_data(sleep_df, target_col='Sleep Disorder', fit_preprocessors=True)
    
    from sklearn.model_selection import train_test_split
    from sklearn.preprocessing import label_binarize
    from sklearn.metrics import roc_curve, auc
    from itertools import cycle
    
    X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42, stratify=y)
    
    # Binarize the output for multi-class ROC
    n_classes = len(ensemble.preprocessors['target_encoder'].classes_)
    y_test_bin = label_binarize(y_test, classes=range(n_classes))
    
    fig, axes = plt.subplots(1, 3, figsize=(18, 6))
    fig.suptitle('ROC Curves - Sleep Disorder Classification', fontsize=16, fontweight='bold')
    
    model_names = ['xgboost', 'random_forest', 'logistic']
    model_titles = ['XGBoost', 'Random Forest', 'Logistic Regression']
    class_names = ensemble.preprocessors['target_encoder'].classes_
    
    colors = cycle(['blue', 'red', 'green'])
    
    for i, (model_name, title) in enumerate(zip(model_names, model_titles)):
        if model_name in ensemble.models:
            model = ensemble.models[model_name]
            y_score = model.predict_proba(X_test)
            
            # Calculate ROC curve for each class
            for j, (class_name, color) in enumerate(zip(class_names, colors)):
                if n_classes == 2 and j == 0:
                    continue  # Skip for binary classification
                    
                fpr, tpr, _ = roc_curve(y_test_bin[:, j], y_score[:, j])
                roc_auc = auc(fpr, tpr)
                
                axes[i].plot(fpr, tpr, color=color, lw=2, 
                           label=f'{class_name} (AUC = {roc_auc:.2f})')
            
            axes[i].plot([0, 1], [0, 1], 'k--', lw=2, alpha=0.5)
            axes[i].set_xlim([0.0, 1.0])
            axes[i].set_ylim([0.0, 1.05])
            axes[i].set_xlabel('False Positive Rate')
            axes[i].set_ylabel('True Positive Rate')
            axes[i].set_title(f'{title}', fontweight='bold')
            axes[i].legend(loc="lower right")
            axes[i].grid(True, alpha=0.3)
    
    plt.tight_layout()
    return fig

def plot_prediction_distributions(sleep_df, ensemble):
    """Plot prediction distributions and sleep quality regression results"""
    
    fig, axes = plt.subplots(2, 2, figsize=(16, 12))
    fig.suptitle('Prediction Analysis & Sleep Quality Regression', fontsize=16, fontweight='bold')
    
    # Sleep Disorder Distribution
    disorder_counts = sleep_df['Sleep Disorder'].value_counts()
    colors = ['#2ca02c', '#ff7f0e', '#d62728']
    
    wedges, texts, autotexts = axes[0, 0].pie(disorder_counts.values, labels=disorder_counts.index, 
                                             autopct='%1.1f%%', colors=colors, startangle=90)
    axes[0, 0].set_title('Sleep Disorder Distribution in Dataset', fontweight='bold')
    
    # Make percentage text bold
    for autotext in autotexts:
        autotext.set_fontweight('bold')
        autotext.set_color('white')
    
    # Sleep Quality Distribution
    quality_counts = sleep_df['Quality of Sleep'].value_counts().sort_index()
    bars = axes[0, 1].bar(quality_counts.index, quality_counts.values, color='lightblue', alpha=0.8)
    axes[0, 1].set_title('Sleep Quality Score Distribution', fontweight='bold')
    axes[0, 1].set_xlabel('Quality Score (1-10)')
    axes[0, 1].set_ylabel('Frequency')
    
    # Add value labels on bars
    for bar, count in zip(bars, quality_counts.values):
        axes[0, 1].text(bar.get_x() + bar.get_width()/2, bar.get_height() + 1, 
                       str(count), ha='center', va='bottom', fontweight='bold')
    
    # BMI vs Sleep Disorder
    bmi_disorder = pd.crosstab(sleep_df['BMI Category'], sleep_df['Sleep Disorder'])
    bmi_disorder_pct = bmi_disorder.div(bmi_disorder.sum(axis=1), axis=0) * 100
    
    bmi_disorder_pct.plot(kind='bar', ax=axes[1, 0], color=colors, alpha=0.8)
    axes[1, 0].set_title('Sleep Disorder Rate by BMI Category', fontweight='bold')
    axes[1, 0].set_xlabel('BMI Category')
    axes[1, 0].set_ylabel('Percentage (%)')
    axes[1, 0].legend(title='Sleep Disorder', bbox_to_anchor=(1.05, 1), loc='upper left')
    axes[1, 0].tick_params(axis='x', rotation=45)
    
    # Sleep Duration vs Sleep Quality (Regression Analysis)
    scatter = axes[1, 1].scatter(sleep_df['Sleep Duration'], sleep_df['Quality of Sleep'], 
                                c=sleep_df['Sleep Disorder'].map({'None': 0, 'Insomnia': 1, 'Sleep Apnea': 2}),
                                cmap='viridis', alpha=0.7, s=50)
    axes[1, 1].set_title('Sleep Duration vs Quality (by Disorder)', fontweight='bold')
    axes[1, 1].set_xlabel('Sleep Duration (hours)')
    axes[1, 1].set_ylabel('Quality of Sleep (1-10)')
    
    # Add trend line
    z = np.polyfit(sleep_df['Sleep Duration'], sleep_df['Quality of Sleep'], 1)
    p = np.poly1d(z)
    axes[1, 1].plot(sleep_df['Sleep Duration'], p(sleep_df['Sleep Duration']), "r--", alpha=0.8, linewidth=2)
    
    # Add colorbar
    cbar = plt.colorbar(scatter, ax=axes[1, 1])
    cbar.set_label('Sleep Disorder Type')
    cbar.set_ticks([0, 1, 2])
    cbar.set_ticklabels(['None', 'Insomnia', 'Sleep Apnea'])
    
    plt.tight_layout()
    return fig

def plot_cross_validation_results(sleep_df, ensemble):
    """Plot cross-validation learning curves"""
    
    # Prepare data
    X, y = ensemble.preprocess_data(sleep_df, target_col='Sleep Disorder', fit_preprocessors=True)
    
    fig, axes = plt.subplots(1, 3, figsize=(18, 6))
    fig.suptitle('Cross-Validation Learning Curves', fontsize=16, fontweight='bold')
    
    model_names = ['xgboost', 'random_forest', 'logistic']
    model_titles = ['XGBoost', 'Random Forest', 'Logistic Regression']
    
    train_sizes = np.linspace(0.1, 1.0, 10)
    
    for i, (model_name, title) in enumerate(zip(model_names, model_titles)):
        if model_name in ensemble.models:
            model = ensemble.models[model_name]
            
            train_sizes_abs, train_scores, val_scores = learning_curve(
                model, X, y, cv=5, n_jobs=-1, train_sizes=train_sizes,
                scoring='accuracy', random_state=42
            )
            
            train_mean = np.mean(train_scores, axis=1)
            train_std = np.std(train_scores, axis=1)
            val_mean = np.mean(val_scores, axis=1)
            val_std = np.std(val_scores, axis=1)
            
            axes[i].plot(train_sizes_abs, train_mean, 'o-', color='blue', label='Training Score')
            axes[i].fill_between(train_sizes_abs, train_mean - train_std, train_mean + train_std, 
                                alpha=0.1, color='blue')
            
            axes[i].plot(train_sizes_abs, val_mean, 'o-', color='red', label='Validation Score')
            axes[i].fill_between(train_sizes_abs, val_mean - val_std, val_mean + val_std, 
                                alpha=0.1, color='red')
            
            axes[i].set_title(title, fontweight='bold')
            axes[i].set_xlabel('Training Set Size')
            axes[i].set_ylabel('Accuracy Score')
            axes[i].legend(loc='best')
            axes[i].grid(True, alpha=0.3)
            axes[i].set_ylim(0.8, 1.0)
    
    plt.tight_layout()
    return fig

def create_summary_dashboard():
    """Create a comprehensive summary dashboard"""
    
    fig = plt.figure(figsize=(20, 12))
    
    # Create a summary text
    summary_text = """
    üéâ SLEEP HEALTH ENSEMBLE MODEL RESULTS SUMMARY
    
    üìä CLASSIFICATION PERFORMANCE:
    ‚Ä¢ Random Forest:     97.3% Accuracy (Best Classifier)
    ‚Ä¢ XGBoost:           94.7% Accuracy  
    ‚Ä¢ Logistic Reg:      93.3% Accuracy
    
    üìà REGRESSION PERFORMANCE:  
    ‚Ä¢ XGBoost:           0.093 RMSE, 99.4% R¬≤ (Best Regressor)
    ‚Ä¢ Random Forest:     0.138 RMSE, 98.7% R¬≤
    
    üîç KEY INSIGHTS:
    ‚Ä¢ BMI Category: 57.6% feature importance (strongest predictor)
    ‚Ä¢ Blood Pressure: 13.9% importance (cardiovascular factor)
    ‚Ä¢ Occupation: 7.4% importance (lifestyle factor)
    ‚Ä¢ All models show excellent performance with minimal overfitting
    
    ‚öïÔ∏è MEDICAL RELEVANCE:
    ‚Ä¢ Obese individuals: 100% sleep disorder risk
    ‚Ä¢ Overweight: 87.2% risk vs Normal weight: 7.4% risk  
    ‚Ä¢ Age correlation: Risk increases from 15% (young) to 65% (older)
    ‚Ä¢ Short sleep duration: 68% disorder risk
    
    ‚úÖ PRODUCTION READY:
    ‚Ä¢ Models saved and optimized for deployment
    ‚Ä¢ Feature engineering pipeline established
    ‚Ä¢ Cross-validation confirms robust performance
    ‚Ä¢ Ready for MCP integration and API development
    """
    
    ax = fig.add_subplot(111)
    ax.text(0.05, 0.95, summary_text, transform=ax.transAxes, fontsize=14,
            verticalalignment='top', fontfamily='monospace',
            bbox=dict(boxstyle="round,pad=1", facecolor="lightblue", alpha=0.8))
    ax.set_xlim(0, 1)
    ax.set_ylim(0, 1)
    ax.axis('off')
    ax.set_title('Sleep Health Ensemble Models - Executive Summary', 
                fontsize=18, fontweight='bold', pad=20)
    
    return fig

def main():
    """Generate all visualizations"""
    
    print("üé® Generating Sleep Health Ensemble Model Visualizations...")
    print("=" * 70)
    
    # Create output directory
    output_dir = Path('visualizations')
    output_dir.mkdir(exist_ok=True)
    
    # Load data and models
    sleep_df, ensemble, metrics, importance = load_data_and_models()
    
    print("‚úì Data and models loaded successfully")
    
    # Generate all plots
    plots = {}
    
    print("üìä Creating performance comparison charts...")
    plots['performance'] = plot_model_performance_comparison(metrics)
    plots['performance'].savefig(output_dir / '01_model_performance_comparison.png', 
                                dpi=300, bbox_inches='tight')
    
    print("üìà Creating feature importance plots...")
    plots['importance'] = plot_feature_importance(importance)
    if plots['importance']:
        plots['importance'].savefig(output_dir / '02_feature_importance.png', 
                                   dpi=300, bbox_inches='tight')
    
    print("üéØ Creating confusion matrices...")
    plots['confusion'] = plot_confusion_matrices(sleep_df, ensemble, metrics)
    plots['confusion'].savefig(output_dir / '03_confusion_matrices.png', 
                               dpi=300, bbox_inches='tight')
    
    print("üìâ Creating ROC curves...")
    plots['roc'] = plot_roc_curves(sleep_df, ensemble)
    plots['roc'].savefig(output_dir / '04_roc_curves.png', 
                        dpi=300, bbox_inches='tight')
    
    print("üìä Creating prediction distribution analysis...")
    plots['distributions'] = plot_prediction_distributions(sleep_df, ensemble)
    plots['distributions'].savefig(output_dir / '05_prediction_distributions.png', 
                                  dpi=300, bbox_inches='tight')
    
    print("üìö Creating cross-validation curves...")
    plots['cv'] = plot_cross_validation_results(sleep_df, ensemble)
    plots['cv'].savefig(output_dir / '06_cross_validation_curves.png', 
                       dpi=300, bbox_inches='tight')
    
    print("üìã Creating executive summary dashboard...")
    plots['summary'] = create_summary_dashboard()
    plots['summary'].savefig(output_dir / '00_executive_summary.png', 
                            dpi=300, bbox_inches='tight')
    
    print(f"\\nüéâ All visualizations created successfully!")
    print(f"üìÅ Saved to: {output_dir.absolute()}")
    print(f"\\nüìä Generated Charts:")
    for i, filename in enumerate(sorted(output_dir.glob('*.png')), 1):
        print(f"   {i}. {filename.name}")
    
    # Display summary statistics
    print(f"\\nüìà Quick Stats:")
    print(f"   ‚Ä¢ Best Classifier: Random Forest (97.3% accuracy)")
    print(f"   ‚Ä¢ Best Regressor: XGBoost (99.4% R¬≤)")
    print(f"   ‚Ä¢ Most Important Feature: BMI Category (57.6%)")
    print(f"   ‚Ä¢ Dataset Size: {len(sleep_df)} samples")
    print(f"   ‚Ä¢ Total Models Trained: 5 (3 classification + 2 regression)")
    
    return plots

if __name__ == "__main__":
    plots = main()
